{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a09bdb8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e81e79",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "df = pd.read_csv('./data/transformed_hcf_scores.csv')\n",
    "\n",
    "categorical_cols = ['Sector', 'Industry']\n",
    "numeric_cols = ['MarketCap',\n",
    "                'Direct Management - hcf score',\n",
    "                'Emotional Connection - hcf score',\n",
    "                'Engagement - hcf score',\n",
    "                'Extrinsic - hcf score',\n",
    "                'Innovation - hcf score',\n",
    "                'Organizational Alignment - hcf score',\n",
    "                'Organizational Effectiveness - hcf score']\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "    ('num', StandardScaler(), numeric_cols)\n",
    "])\n",
    "\n",
    "k = 5\n",
    "pipeline = Pipeline([\n",
    "    ('prep', preprocessor),\n",
    "    ('cluster', KMeans(n_clusters=k, random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "df = df.dropna(subset=categorical_cols + numeric_cols)\n",
    "labels = pipeline.fit_predict(df[categorical_cols + numeric_cols])\n",
    "\n",
    "\n",
    "df['cluster'] = labels\n",
    "\n",
    "\n",
    "feat_matrix = pipeline.named_steps['prep'].transform(df[categorical_cols + numeric_cols])\n",
    "sil_score = silhouette_score(feat_matrix, labels)\n",
    "print(f'Chosen k={k}, silhouette score = {sil_score:.3f}')\n",
    "\n",
    "\n",
    "best_k, best_score = 0, -1\n",
    "for k_try in range(2, 11):\n",
    "    km = KMeans(n_clusters=k_try, random_state=42)\n",
    "    lbl = km.fit_predict(feat_matrix)\n",
    "    score = silhouette_score(feat_matrix, lbl)\n",
    "    if score > best_score:\n",
    "        best_k, best_score = k_try, score\n",
    "print(f'Best k by silhouette: {best_k} (score={best_score:.3f})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf13793",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df['cluster'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db54c4c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cluster_companies = []\n",
    "for i in range(5):\n",
    "  cluster_companies.append(df.loc[df['cluster'] == i, ['Code','Name', 'Sector']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67b332a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cluster_companies[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dca26f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "feat_names = pipeline.named_steps['prep'] \\\n",
    "                .get_feature_names_out()\n",
    "\n",
    "centers = pipeline.named_steps['cluster'].cluster_centers_\n",
    "\n",
    "import pandas as pd\n",
    "centroids_df = pd.DataFrame(centers, columns=feat_names)\n",
    "\n",
    "for i, row in centroids_df.iterrows():\n",
    "    print(f\"\\nCluster {i} top features:\")\n",
    "    print(row.abs().sort_values(ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda6bfc2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "text17_df = pd.read_csv(\n",
    "    \"./data/gd_sample_sy2017.csv\",\n",
    "    engine=\"python\",\n",
    "    on_bad_lines=\"skip\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4ee040",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "text17_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02afdefa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_cols = [\n",
    "    'Helpful Count', 'Not Helpful Count',\n",
    "    'Rating: Overall', 'Rating: Work/Life Balance',\n",
    "    'Rating: Culture & Values', 'Rating: Career Opportunities',\n",
    "    'Rating: Comp & Benefits', 'Rating: Senior Management',\n",
    "    'Rating: Diversity & Inclusion', 'Length of Employment'\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    'Sector', 'Industry', 'GICS Sector', 'Exchange', 'Gender'\n",
    "]\n",
    "\n",
    "text_cols = [\n",
    "    'Summary', 'Description', 'PROs', 'CONs', 'Advice to Management'\n",
    "]\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler',  StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "\n",
    "def combine_text(X):\n",
    "    return X[text_cols].fillna(\"\").agg(\" \".join, axis=1)\n",
    "\n",
    "text_transformer = Pipeline([\n",
    "    ('selector', FunctionTransformer(combine_text, validate=False)),\n",
    "    ('tfidf', TfidfVectorizer(max_features=5_000, stop_words='english'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols),\n",
    "    ('txt', text_transformer, text_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('svd',     TruncatedSVD(n_components=50, random_state=42)),\n",
    "    ('kmeans',  KMeans(n_clusters=5, random_state=42))\n",
    "])\n",
    "\n",
    "cluster_labels = pipeline.fit_predict(text17_df)\n",
    "\n",
    "text17_df['cluster'] = cluster_labels\n",
    "\n",
    "print(text17_df['cluster'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d40ec7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "t_cluster_companies = []\n",
    "for i in range(5):\n",
    "  t_cluster_companies.append(text17_df.loc[text17_df['cluster'] == i, ['Ticker Symbol','Company', 'ICB Sector']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffe93fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "text17_df[\"Company\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f0e817",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "hcf_df = pd.read_csv('/content/drive/MyDrive/finM/transformed_hcf_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac9e4d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "hcf_score_cols = [\n",
    "    'Direct Management - hcf score',\n",
    "    'Emotional Connection - hcf score',\n",
    "    'Engagement - hcf score',\n",
    "    'Extrinsic - hcf score',\n",
    "    'Innovation - hcf score',\n",
    "    'Organizational Alignment - hcf score',\n",
    "    'Organizational Effectiveness - hcf score'\n",
    "]\n",
    "hcf_scores = hcf_df[hcf_score_cols].dropna().copy().reset_index(drop=True)\n",
    "\n",
    "pca_hcf = PCA(n_components=5, random_state=0)  # adjust n_components as needed\n",
    "Z_hcf = pca_hcf.fit_transform(hcf_scores)  # shape (n_companies, latent_dim)\n",
    "\n",
    "\n",
    "\n",
    "text_fields = ['Advice to Management', 'PROs', 'CONs', 'Summary', 'Description']\n",
    "text17_df['combined_text'] = text17_df[text_fields].fillna('').agg(' '.join, axis=1)\n",
    "\n",
    "rating_cols = [\n",
    "    'Rating: Overall', 'Rating: Work/Life Balance', 'Rating: Culture & Values',\n",
    "    'Rating: Career Opportunities', 'Rating: Comp & Benefits', 'Rating: Senior Management',\n",
    "    'Rating: Diversity & Inclusion'\n",
    "]\n",
    "numeric_cols = rating_cols + ['Fprob_PROs', 'Fprob_CONs', 'Fprob']\n",
    "\n",
    "review_numeric = text17_df[numeric_cols].fillna(0)\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words='english')\n",
    "svd = TruncatedSVD(n_components=50, random_state=0)\n",
    "text_mat = tfidf.fit_transform(text17_df['combined_text'])\n",
    "text_latent = svd.fit_transform(text_mat)  # shape (n_reviews, 50)\n",
    "\n",
    "\n",
    "X_review_base = np.hstack([text_latent, review_numeric.to_numpy()])\n",
    "\n",
    "\n",
    "pca_review = PCA(n_components=5, random_state=0)\n",
    "Z_review = pca_review.fit_transform(X_review_base)  # shape (n_reviews, latent_dim)\n",
    "\n",
    "aux_fields = ['Sector', 'Industry']\n",
    "hcf_df_aux = hcf_df.loc[hcf_scores.index, aux_fields].fillna('missing').copy()\n",
    "text17_df_aux = text17_df[aux_fields].fillna('missing').copy()\n",
    "\n",
    "combined_aux = pd.concat([text17_df_aux, hcf_df_aux], axis=0).reset_index(drop=True)\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "ohe.fit(combined_aux)\n",
    "\n",
    "X_aux_reviews = ohe.transform(text17_df_aux)    # shape (n_reviews, ...)\n",
    "X_aux_hcf = ohe.transform(hcf_df_aux)           # shape matches Z_hcf now\n",
    "\n",
    "reg_aux2hcf = Ridge(alpha=1.0)\n",
    "reg_aux2hcf.fit(X_aux_hcf, Z_hcf)  # no length mismatch\n",
    "\n",
    "reg_aux2rev = Ridge(alpha=1.0)\n",
    "reg_aux2rev.fit(X_aux_reviews, Z_review)  # maps shared aux to review latent\n",
    "\n",
    "# Pick a set of representative auxiliary vectors to create pseudo-pairs.\n",
    "# Here: use the unique auxiliary combinations seen in hcf_df (could also cluster if too many)\n",
    "Z_hcf_from_aux = reg_aux2hcf.predict(X_aux_hcf)    # predicted HCF latent from its own aux\n",
    "Z_rev_from_aux = reg_aux2rev.predict(X_aux_hcf)    # predicted review latent for same aux\n",
    "\n",
    "R, scale = orthogonal_procrustes(Z_rev_from_aux, Z_hcf_from_aux)  # orthogonal matrix\n",
    "\n",
    "def predict_hcf_from_review(review_row):\n",
    "    text = \" \".join([str(review_row.get(f, \"\")) for f in text_fields])\n",
    "    text_vec = tfidf.transform([text])\n",
    "    text_red = svd.transform(text_vec) \n",
    "    numeric = np.array([review_row.get(c, 0) for c in numeric_cols]).reshape(1, -1)\n",
    "    X_base = np.hstack([text_red, numeric]) \n",
    "    z_rev = pca_review.transform(X_base)     \n",
    "    z_hcf_aligned = z_rev @ R\n",
    "    hcf_pred = pca_hcf.inverse_transform(z_hcf_aligned) \n",
    "    return pd.Series(hcf_pred.flatten(), index=hcf_score_cols)\n",
    "\n",
    "\n",
    "\n",
    "Z_rev_from_aux_aligned = Z_rev_from_aux @ R  \n",
    "reconstructed_hcf_scores = pca_hcf.inverse_transform(Z_rev_from_aux_aligned)\n",
    "original_hcf_scores = hcf_scores.values \n",
    "\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(original_hcf_scores, reconstructed_hcf_scores))\n",
    "print(f\"Pseudo alignment RMSE (HCF recon from aux via review path): {rmse:.4f}\")\n",
    "\n",
    "def bootstrap_alignment(n_boot=50):\n",
    "    rmses = []\n",
    "    for _ in range(n_boot):\n",
    "        idx_hcf = np.random.choice(range(X_aux_hcf.shape[0]), size=X_aux_hcf.shape[0], replace=True)\n",
    "        idx_rev = np.random.choice(range(X_aux_reviews.shape[0]), size=X_aux_reviews.shape[0], replace=True)\n",
    "\n",
    "\n",
    "        reg_h = Ridge(alpha=1.0).fit(X_aux_hcf[idx_hcf], Z_hcf[idx_hcf])\n",
    "        reg_r = Ridge(alpha=1.0).fit(X_aux_reviews[idx_rev], Z_review[idx_rev])\n",
    "\n",
    "        Zh_from_aux = reg_h.predict(X_aux_hcf[idx_hcf])\n",
    "        Zr_from_aux = reg_r.predict(X_aux_hcf[idx_hcf]) \n",
    "        R_boot, _ = orthogonal_procrustes(Zr_from_aux, Zh_from_aux)\n",
    "        aligned = Zr_from_aux @ R_boot\n",
    "        recon = pca_hcf.inverse_transform(aligned)\n",
    "        orig = hcf_scores.values[idx_hcf]\n",
    "        rmses.append(np.sqrt(mean_squared_error(orig, recon)))\n",
    "    return np.mean(rmses), np.std(rmses)\n",
    "\n",
    "mean_rmse, std_rmse = bootstrap_alignment(30)\n",
    "print(f\"Bootstrap pseudo alignment RMSE: {mean_rmse:.4f} ± {std_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc91e273",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "var = np.var(hcf_scores.values, axis=0, ddof=1)\n",
    "rmse_per_score = np.sqrt(mean_squared_error(original_hcf_scores, reconstructed_hcf_scores, multioutput='raw_values'))\n",
    "print(\"Explained fraction per score:\", 1 - (rmse_per_score**2) / var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c85b3f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "hcf_score_cols = [\n",
    "    'Direct Management - hcf score',\n",
    "    'Emotional Connection - hcf score',\n",
    "    'Engagement - hcf score',\n",
    "    'Extrinsic - hcf score',\n",
    "    'Innovation - hcf score',\n",
    "    'Organizational Alignment - hcf score',\n",
    "    'Organizational Effectiveness - hcf score'\n",
    "]\n",
    "text_fields = ['Advice to Management', 'PROs', 'CONs', 'Summary', 'Description']\n",
    "rating_cols = [\n",
    "    'Rating: Overall', 'Rating: Work/Life Balance', 'Rating: Culture & Values',\n",
    "    'Rating: Career Opportunities', 'Rating: Comp & Benefits', 'Rating: Senior Management',\n",
    "    'Rating: Diversity & Inclusion'\n",
    "]\n",
    "numeric_cols = rating_cols + ['Fprob_PROs', 'Fprob_CONs', 'Fprob']\n",
    "aux_fields = ['Sector', 'Industry']\n",
    "\n",
    "hcf_valid = hcf_df.dropna(subset=hcf_score_cols).copy().reset_index(drop=True)\n",
    "hcf_scores = hcf_valid[hcf_score_cols].copy()  # (n_hcf, 7)\n",
    "hcf_df_aux = hcf_valid[aux_fields].fillna('missing').copy()  # aligned with hcf_scores\n",
    "\n",
    "text17_df['combined_text'] = text17_df[text_fields].fillna('').agg(' '.join, axis=1)\n",
    "for c in numeric_cols:\n",
    "    if c not in text17_df.columns:\n",
    "        text17_df[c] = 0.0\n",
    "review_numeric = text17_df[numeric_cols].fillna(0)\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words='english')\n",
    "svd = TruncatedSVD(n_components=50, random_state=0)\n",
    "text_mat = tfidf.fit_transform(text17_df['combined_text'])\n",
    "text_latent = svd.fit_transform(text_mat)  # (n_reviews, 50)\n",
    "\n",
    "X_review_base = np.hstack([text_latent, review_numeric.to_numpy()])  # (n_reviews, D)\n",
    "\n",
    "\n",
    "text17_df_aux = text17_df[aux_fields].fillna('missing').copy()\n",
    "combined_aux = pd.concat([text17_df_aux, hcf_df_aux], axis=0).reset_index(drop=True)\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "ohe.fit(combined_aux)\n",
    "\n",
    "X_aux_reviews = ohe.transform(text17_df_aux)  # (n_reviews, ...)\n",
    "X_aux_hcf = ohe.transform(hcf_df_aux)         # (n_hcf, ...), now aligned with hcf_scores\n",
    "\n",
    "\n",
    "def explained_fraction(original, recon):\n",
    "    var = np.var(original, axis=0, ddof=1)\n",
    "    rmse_per_score = np.sqrt(mean_squared_error(original, recon, multioutput='raw_values'))\n",
    "    return 1 - (rmse_per_score**2) / var  # can be negative\n",
    "\n",
    "best = {}\n",
    "results = []\n",
    "for hcf_dim in [3, 5, 7]:\n",
    "    pca_hcf = PCA(n_components=hcf_dim, random_state=0)\n",
    "    Z_hcf = pca_hcf.fit_transform(hcf_scores)  # (n_hcf, hcf_dim)\n",
    "\n",
    "    reg_aux2hcf = Ridge(alpha=1.0)\n",
    "    reg_aux2hcf.fit(X_aux_hcf, Z_hcf)\n",
    "    Z_hcf_from_aux = reg_aux2hcf.predict(X_aux_hcf)  # pseudo HCF latent\n",
    "\n",
    "    for rev_dim in [5, 10, 15]:\n",
    "        pca_review = PCA(n_components=rev_dim, random_state=0)\n",
    "        Z_review = pca_review.fit_transform(X_review_base)  # (n_reviews, rev_dim)\n",
    "\n",
    "        reg_aux2rev = Ridge(alpha=1.0)\n",
    "        reg_aux2rev.fit(X_aux_reviews, Z_review)\n",
    "        Z_rev_from_aux = reg_aux2rev.predict(X_aux_hcf)  # using HCF-side aux\n",
    "\n",
    "        cca_n = min(hcf_dim, rev_dim, 5)\n",
    "        cca = CCA(n_components=cca_n, max_iter=500)\n",
    "        U, V = cca.fit_transform(Z_rev_from_aux, Z_hcf_from_aux)  # (n_hcf, cca_n)\n",
    "\n",
    "        reg_u2hcf = Ridge()\n",
    "        reg_u2hcf.fit(U, Z_hcf_from_aux)\n",
    "\n",
    "        U_full = cca.transform(Z_rev_from_aux)\n",
    "        Z_hcf_pred_latent = reg_u2hcf.predict(U_full)\n",
    "        hcf_reconstructed = pca_hcf.inverse_transform(Z_hcf_pred_latent)\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(hcf_scores.values, hcf_reconstructed))\n",
    "        expl_frac = explained_fraction(hcf_scores.values, hcf_reconstructed)\n",
    "        avg_expl = expl_frac.mean()\n",
    "\n",
    "        results.append({\n",
    "            'hcf_dim': hcf_dim,\n",
    "            'rev_dim': rev_dim,\n",
    "            'cca_n': cca_n,\n",
    "            'rmse': rmse,\n",
    "            'avg_explained_frac': avg_expl,\n",
    "            'per_score_explained_frac': expl_frac,\n",
    "            'pca_hcf': pca_hcf,\n",
    "            'pca_review': pca_review,\n",
    "            'reg_aux2hcf': reg_aux2hcf,\n",
    "            'reg_aux2rev': reg_aux2rev,\n",
    "            'cca': cca,\n",
    "            'reg_u2hcf': reg_u2hcf,\n",
    "        })\n",
    "        if 'best_avg' not in best or avg_expl > best['best_avg']:\n",
    "            best.update({\n",
    "                'best_avg': avg_expl,\n",
    "                'hcf_dim': hcf_dim,\n",
    "                'rev_dim': rev_dim,\n",
    "                'cca_n': cca_n,\n",
    "                'pipeline': results[-1]\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pipe = best['pipeline']\n",
    "pca_hcf = pipe['pca_hcf']\n",
    "pca_review = pipe['pca_review']\n",
    "reg_aux2rev = pipe['reg_aux2rev']\n",
    "cca = pipe['cca']\n",
    "reg_u2hcf = pipe['reg_u2hcf']\n",
    "\n",
    "def predict_hcf_from_review_latent(review_row):\n",
    "\n",
    "    text = \" \".join([str(review_row.get(f, \"\")) for f in text_fields])\n",
    "    text_vec = tfidf.transform([text])\n",
    "    text_red = svd.transform(text_vec)\n",
    "    numeric = np.array([review_row.get(c, 0) for c in numeric_cols]).reshape(1, -1)\n",
    "    X_base_new = np.hstack([text_red, numeric])\n",
    "\n",
    "\n",
    "    aux_vec = ohe.transform(pd.DataFrame([{\n",
    "        'Sector': review_row.get('Sector', 'missing'),\n",
    "        'Industry': review_row.get('Industry', 'missing')\n",
    "    }]))\n",
    "    Z_rev_from_aux = reg_aux2rev.predict(aux_vec)\n",
    "    U_proj = cca.transform(Z_rev_from_aux)\n",
    "    z_hcf_latent = reg_u2hcf.predict(U_proj)\n",
    "    hcf_pred = pca_hcf.inverse_transform(z_hcf_latent)\n",
    "    return pd.Series(hcf_pred.flatten(), index=hcf_score_cols)\n",
    "\n",
    "\n",
    "example = text17_df.iloc[0]\n",
    "\n",
    "\n",
    "\n",
    "def compute_explained_fraction_of_pipeline():\n",
    "    hcf_recon = pca_hcf.inverse_transform(\n",
    "        reg_u2hcf.predict(cca.transform(reg_aux2rev.predict(X_aux_hcf)))\n",
    "    )\n",
    "    return explained_fraction(hcf_scores.values, hcf_recon)\n",
    "\n",
    "print(\"Final explained fraction per score:\", compute_explained_fraction_of_pipeline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dce24c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "hcf_score_cols = [\n",
    "    'Direct Management - hcf score',\n",
    "    'Emotional Connection - hcf score',\n",
    "    'Engagement - hcf score',\n",
    "    'Extrinsic - hcf score',\n",
    "    'Innovation - hcf score',\n",
    "    'Organizational Alignment - hcf score',\n",
    "    'Organizational Effectiveness - hcf score'\n",
    "]\n",
    "text_fields = ['Advice to Management', 'PROs', 'CONs', 'Summary', 'Description']\n",
    "rating_cols = [\n",
    "    'Rating: Overall', 'Rating: Work/Life Balance', 'Rating: Culture & Values',\n",
    "    'Rating: Career Opportunities', 'Rating: Comp & Benefits', 'Rating: Senior Management',\n",
    "    'Rating: Diversity & Inclusion'\n",
    "]\n",
    "numeric_cols = rating_cols + ['Fprob_PROs', 'Fprob_CONs', 'Fprob']\n",
    "aux_cat_fields = ['Sector', 'Industry']\n",
    "\n",
    "has_survey = 'SurveyYear' in text17_df.columns\n",
    "\n",
    "\n",
    "hcf_valid = hcf_df.dropna(subset=hcf_score_cols).reset_index(drop=True)\n",
    "hcf_scores = hcf_valid[hcf_score_cols].copy()  # (n_hcf, 7)\n",
    "hcf_aux_cat = hcf_valid[aux_cat_fields].fillna('missing').copy()\n",
    "# placeholder for missing review-like metadata\n",
    "# We'll create review_len proxy as mean from reviews later\n",
    "\n",
    "\n",
    "text17_df['combined_text'] = text17_df[text_fields].fillna('').agg(' '.join, axis=1)\n",
    "for c in numeric_cols:\n",
    "    if c not in text17_df.columns:\n",
    "        text17_df[c] = 0.0\n",
    "review_numeric = text17_df[numeric_cols].fillna(0)\n",
    "\n",
    "\n",
    "tfidf_for_review = TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words='english')\n",
    "svd = TruncatedSVD(n_components=50, random_state=0)\n",
    "text_mat = tfidf_for_review.fit_transform(text17_df['combined_text'])\n",
    "text_latent = svd.fit_transform(text_mat)  # (n_reviews, 50)\n",
    "X_review_base = np.hstack([text_latent, review_numeric.to_numpy()])\n",
    "\n",
    "\n",
    "tfidf_topic = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "W = tfidf_topic.fit_transform(text17_df['combined_text'])\n",
    "n_topics = 10\n",
    "nmf = NMF(n_components=n_topics, random_state=0, init='nndsvda', max_iter=500)\n",
    "topic_weights = nmf.fit_transform(W)  # (n_reviews, n_topics)\n",
    "topic_weights = topic_weights / (topic_weights.sum(axis=1, keepdims=True) + 1e-9)  # normalize\n",
    "\n",
    "\n",
    "text17_df['review_len'] = text17_df['combined_text'].str.split().apply(len)\n",
    "text17_df['log_review_len'] = np.log1p(text17_df['review_len'])\n",
    "\n",
    "\n",
    "aux_reviews_df = pd.DataFrame({\n",
    "    'Sector': text17_df['Sector'].fillna('missing'),\n",
    "    'Industry': text17_df['Industry'].fillna('missing'),\n",
    "    'log_review_len': text17_df['log_review_len'].fillna(0),\n",
    "})\n",
    "if has_survey:\n",
    "    aux_reviews_df['SurveyYear'] = text17_df['SurveyYear'].astype(str).fillna('missing')\n",
    "else:\n",
    "    aux_reviews_df['SurveyYear'] = 'missing'\n",
    "\n",
    "\n",
    "rev_topics_df = pd.concat([\n",
    "    text17_df[['Sector', 'Industry']].reset_index(drop=True),\n",
    "    pd.DataFrame(topic_weights, columns=[f\"topic_{i}\" for i in range(n_topics)])\n",
    "], axis=1)\n",
    "group_mean_topics = rev_topics_df.groupby(['Sector', 'Industry']).mean().reset_index()\n",
    "\n",
    "hcf_topic_merge = hcf_aux_cat.merge(\n",
    "    group_mean_topics,\n",
    "    on=['Sector', 'Industry'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "global_topic_mean = topic_weights.mean(axis=0)\n",
    "\n",
    "\n",
    "for i in range(n_topics):\n",
    "    col = f\"topic_{i}\"\n",
    "    if col not in hcf_topic_merge:\n",
    "        hcf_topic_merge[col] = global_topic_mean[i]\n",
    "    hcf_topic_merge[col] = hcf_topic_merge[col].fillna(global_topic_mean[i])\n",
    "\n",
    "hcf_topic_weights = hcf_topic_merge[[f\"topic_{i}\" for i in range(n_topics)]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mean_log_len = text17_df['log_review_len'].mean()\n",
    "hcf_aux_df = pd.DataFrame({\n",
    "    'Sector': hcf_valid['Sector'].fillna('missing'),\n",
    "    'Industry': hcf_valid['Industry'].fillna('missing'),\n",
    "    'log_review_len': np.full(len(hcf_valid), mean_log_len),\n",
    "})\n",
    "if has_survey:\n",
    "    hcf_aux_df['SurveyYear'] = 'missing'\n",
    "else:\n",
    "    hcf_aux_df['SurveyYear'] = 'missing'\n",
    "\n",
    "\n",
    "cat_ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "cat_ohe.fit(pd.concat([aux_reviews_df[['Sector','Industry']], hcf_aux_df[['Sector','Industry']]], axis=0))\n",
    "\n",
    "\n",
    "cat_rev = cat_ohe.transform(aux_reviews_df[['Sector','Industry']])\n",
    "parts_rev = [cat_rev, aux_reviews_df[['log_review_len']].to_numpy(), topic_weights]\n",
    "if has_survey:\n",
    "    year_ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    year_enc_rev = year_ohe.fit_transform(aux_reviews_df[['SurveyYear']])\n",
    "    parts_rev.append(year_enc_rev)\n",
    "X_aux_reviews_enriched = np.hstack(parts_rev)\n",
    "\n",
    "\n",
    "cat_hcf = cat_ohe.transform(hcf_aux_df[['Sector','Industry']])\n",
    "parts_hcf = [cat_hcf, hcf_aux_df[['log_review_len']].to_numpy(), hcf_topic_weights.to_numpy()]\n",
    "if has_survey:\n",
    "    year_enc_hcf = year_ohe.transform(pd.DataFrame({'SurveyYear': ['missing'] * len(hcf_valid)}))\n",
    "    parts_hcf.append(year_enc_hcf)\n",
    "X_aux_hcf_enriched = np.hstack(parts_hcf)\n",
    "\n",
    "\n",
    "def explained_fraction(original, recon):\n",
    "    var = np.var(original, axis=0, ddof=1)\n",
    "    rmse_per_score = np.sqrt(mean_squared_error(original, recon, multioutput='raw_values'))\n",
    "    return 1 - (rmse_per_score**2) / var\n",
    "\n",
    "\n",
    "pca_hcf = PCA(n_components=7, random_state=0) \n",
    "Z_hcf = pca_hcf.fit_transform(hcf_scores) \n",
    "\n",
    "\n",
    "pca_review = PCA(n_components=5, random_state=0)\n",
    "Z_review = pca_review.fit_transform(X_review_base)\n",
    "\n",
    "\n",
    "reg_aux2hcf = Ridge(alpha=1.0)\n",
    "reg_aux2hcf.fit(X_aux_hcf_enriched, Z_hcf)  \n",
    "\n",
    "reg_aux2rev = Ridge(alpha=1.0)\n",
    "reg_aux2rev.fit(X_aux_reviews_enriched, Z_review)  \n",
    "\n",
    "\n",
    "Z_hcf_from_aux = reg_aux2hcf.predict(X_aux_hcf_enriched)    \n",
    "Z_rev_from_aux = reg_aux2rev.predict(X_aux_hcf_enriched)    \n",
    "\n",
    "reg_rev2hcf = Ridge()\n",
    "reg_rev2hcf.fit(Z_rev_from_aux, Z_hcf_from_aux)  \n",
    "Z_hcf_pred_lin = reg_rev2hcf.predict(Z_rev_from_aux)\n",
    "hcf_recon_lin = pca_hcf.inverse_transform(Z_hcf_pred_lin)\n",
    "expl_frac_lin = explained_fraction(hcf_scores.values, hcf_recon_lin)\n",
    "rmse_lin = np.sqrt(mean_squared_error(hcf_scores.values, hcf_recon_lin))\n",
    "\n",
    "cca_n = min(Z_rev_from_aux.shape[1], Z_hcf_from_aux.shape[1], 5)\n",
    "cca = CCA(n_components=cca_n, max_iter=2000)\n",
    "U, V = cca.fit_transform(Z_rev_from_aux, Z_hcf_from_aux)\n",
    "reg_u2hcf = Ridge()\n",
    "reg_u2hcf.fit(U, Z_hcf_from_aux)\n",
    "U_full = cca.transform(Z_rev_from_aux)\n",
    "Z_hcf_pred_cca = reg_u2hcf.predict(U_full)\n",
    "hcf_recon_cca = pca_hcf.inverse_transform(Z_hcf_pred_cca)\n",
    "expl_frac_cca = explained_fraction(hcf_scores.values, hcf_recon_cca)\n",
    "rmse_cca = np.sqrt(mean_squared_error(hcf_scores.values, hcf_recon_cca))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_hcf_for_review_enriched(review_row):\n",
    "    # Build enriched auxiliary for this review\n",
    "    sector = review_row.get('Sector', 'missing')\n",
    "    industry = review_row.get('Industry', 'missing')\n",
    "    log_len = np.log1p(len(str(review_row.get('combined_text', \"\")).split()))\n",
    "    survey = str(review_row.get('SurveyYear', 'missing')) if has_survey else 'missing'\n",
    "\n",
    "\n",
    "    review_text = \" \".join([str(review_row.get(f, \"\")) for f in text_fields])\n",
    "    topic_vec = nmf.transform(tfidf_topic.transform([review_text]))  # (1, n_topics)\n",
    "    topic_vec = topic_vec / (topic_vec.sum() + 1e-9)\n",
    "\n",
    "\n",
    "    cat_part = cat_ohe.transform(pd.DataFrame([{'Sector': sector, 'Industry': industry}]))\n",
    "    parts = [cat_part, np.array([[log_len]]), topic_vec]\n",
    "    if has_survey:\n",
    "        year_part = year_ohe.transform(pd.DataFrame([{'SurveyYear': survey}]))\n",
    "        parts.append(year_part)\n",
    "    aux_review = np.hstack(parts)  # (1, dim)\n",
    "\n",
    "\n",
    "    z_rev_proxy = reg_aux2rev.predict(aux_review)  # (1, latent_review)\n",
    "\n",
    "    z_hcf_lin = reg_rev2hcf.predict(z_rev_proxy)\n",
    "    hcf_lin = pca_hcf.inverse_transform(z_hcf_lin).flatten()\n",
    "\n",
    "    U_proj = cca.transform(z_rev_proxy)\n",
    "    z_hcf_cca = reg_u2hcf.predict(U_proj)\n",
    "    hcf_cca = pca_hcf.inverse_transform(z_hcf_cca).flatten()\n",
    "\n",
    "    return {\n",
    "        'hcf_pred_linear': pd.Series(hcf_lin, index=hcf_score_cols),\n",
    "        'hcf_pred_cca': pd.Series(hcf_cca, index=hcf_score_cols),\n",
    "    }\n",
    "\n",
    "\n",
    "example = text17_df.iloc[0]\n",
    "preds = predict_hcf_for_review_enriched(example)\n",
    "print(\"Example linear-aligned HCF prediction:\\n\", preds['hcf_pred_linear'])\n",
    "print(\"Example CCA-aligned HCF prediction:\\n\", preds['hcf_pred_cca'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfabbc6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "hcf_score_cols = [\n",
    "    'Direct Management - hcf score',\n",
    "    'Emotional Connection - hcf score',\n",
    "    'Engagement - hcf score',\n",
    "    'Extrinsic - hcf score',\n",
    "    'Innovation - hcf score',\n",
    "    'Organizational Alignment - hcf score',\n",
    "    'Organizational Effectiveness - hcf score'\n",
    "]\n",
    "text_fields = ['Advice to Management', 'PROs', 'CONs', 'Summary', 'Description']\n",
    "aux_cat_fields = ['Sector', 'Industry']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    st_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # small and fast\n",
    "    def get_text_embedding(texts):\n",
    "        return st_model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
    "    print(\"Using sentence-transformer embeddings.\")\n",
    "except ImportError:\n",
    "    print(\"sentence_transformers not installed; falling back to TF-IDF + PCA.\")\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "    text17_df['combined_text'] = text17_df[text_fields].fillna('').agg(' '.join, axis=1)\n",
    "    tfidf_mat = tfidf.fit_transform(text17_df['combined_text'])\n",
    "    pca_text = PCA(n_components=128, random_state=0)\n",
    "    text_reduced = pca_text.fit_transform(tfidf_mat.toarray())\n",
    "    def get_text_embedding(texts):\n",
    "        return pca_text.transform(tfidf.transform(texts).toarray())\n",
    "\n",
    "\n",
    "hcf_valid = hcf_df.dropna(subset=hcf_score_cols).reset_index(drop=True)\n",
    "hcf_scores = hcf_valid[hcf_score_cols].to_numpy() \n",
    "\n",
    "pca_hcf = PCA(n_components=5, random_state=1)\n",
    "Z_hcf = pca_hcf.fit_transform(hcf_scores) \n",
    "\n",
    "\n",
    "hcf_valid['aux_key'] = hcf_valid[aux_cat_fields].fillna('missing').agg(\"__\".join, axis=1)\n",
    "text17_df['combined_text'] = text17_df[text_fields].fillna('').agg(' '.join, axis=1)\n",
    "text17_df['aux_key'] = text17_df[aux_cat_fields].fillna('missing').agg(\"__\".join, axis=1)\n",
    "\n",
    "\n",
    "pairs = []\n",
    "k_per_review = 3 \n",
    "for rev_idx, rev in text17_df.iterrows():\n",
    "    rev_sector = str(rev.get('Sector', 'missing')).strip()\n",
    "    rev_ind = str(rev.get('Industry', 'missing')).strip()\n",
    "    def match_score(hcf_row):\n",
    "        score = 0\n",
    "        if str(hcf_row['Sector']).strip() == rev_sector:\n",
    "            score += 1\n",
    "        if str(hcf_row['Industry']).strip() == rev_ind:\n",
    "            score += 1\n",
    "        return score  # 0,1,2\n",
    "\n",
    "    hcf_valid = hcf_valid if 'hcf_valid' in globals() else hcf_df.dropna(subset=hcf_score_cols).reset_index(drop=True)\n",
    "\n",
    "    scores = hcf_valid.apply(match_score, axis=1)\n",
    "    best_idxs = scores[scores > 0].sort_values(ascending=False).index.tolist()\n",
    "    if len(best_idxs) == 0:\n",
    "\n",
    "        sampled_idxs = hcf_valid.sample(n=k_per_review, replace=True, random_state=rev_idx).index.tolist()\n",
    "    else:\n",
    "\n",
    "        if len(best_idxs) > k_per_review:\n",
    "            sampled_idxs = random.sample(best_idxs, k_per_review)\n",
    "        else:\n",
    "            sampled_idxs = best_idxs\n",
    "    for hcf_idx in sampled_idxs:\n",
    "        pairs.append({\n",
    "            'review_idx': rev_idx,\n",
    "            'hcf_idx': hcf_idx,\n",
    "            'rev_sector': rev_sector,\n",
    "            'rev_industry': rev_ind,\n",
    "            'hcf_sector': str(hcf_valid.loc[hcf_idx, 'Sector']).strip(),\n",
    "            'hcf_industry': str(hcf_valid.loc[hcf_idx, 'Industry']).strip(),\n",
    "        })\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs)\n",
    "\n",
    "if pairs_df.empty:\n",
    "    raise RuntimeError(\"No pseudo-pairs could be constructed; verify Sector/Industry columns in both datasets.\")\n",
    "\n",
    "\n",
    "\n",
    "review_texts = text17_df.loc[pairs_df['review_idx'], 'combined_text'].tolist()\n",
    "E_rev = get_text_embedding(review_texts)  \n",
    "\n",
    "E_hcf = Z_hcf[pairs_df['hcf_idx'].to_numpy()]  \n",
    "\n",
    "scaler_rev = StandardScaler().fit(E_rev)\n",
    "scaler_hcf = StandardScaler().fit(E_hcf)\n",
    "E_rev_s = scaler_rev.transform(E_rev)\n",
    "E_hcf_s = scaler_hcf.transform(E_hcf)\n",
    "\n",
    "\n",
    "rev_tensor = torch.tensor(E_rev_s, dtype=torch.float32, device=device)\n",
    "hcf_tensor = torch.tensor(E_hcf_s, dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = F.normalize(x, dim=-1)\n",
    "        return x\n",
    "\n",
    "rev_proj = ProjectionHead(rev_tensor.shape[1]).to(device)\n",
    "hcf_proj = ProjectionHead(hcf_tensor.shape[1]).to(device)\n",
    "\n",
    "\n",
    "def contrastive_loss(z1, z2, temperature=0.1):\n",
    "\n",
    "    batch_size = z1.shape[0]\n",
    "    logits = torch.matmul(z1, z2.T) / temperature  # (B,B)\n",
    "    labels = torch.arange(batch_size, device=device)\n",
    "    loss1 = F.cross_entropy(logits, labels)\n",
    "    loss2 = F.cross_entropy(logits.T, labels)\n",
    "    return 0.5 * (loss1 + loss2)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(list(rev_proj.parameters()) + list(hcf_proj.parameters()), lr=1e-3)\n",
    "n_epochs = 300\n",
    "for epoch in range(n_epochs):\n",
    "    rev_z = rev_proj(rev_tensor)\n",
    "    hcf_z = hcf_proj(hcf_tensor)\n",
    "    loss = contrastive_loss(rev_z, hcf_z)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 60 == 0:\n",
    "        print(f\"[Contrastive] Epoch {epoch+1}/{n_epochs} loss: {loss.item():.4f}\")\n",
    "\n",
    "all_review_texts = text17_df['combined_text'].tolist()\n",
    "E_rev_all = get_text_embedding(all_review_texts)\n",
    "E_rev_all_s = scaler_rev.transform(E_rev_all)\n",
    "with torch.no_grad():\n",
    "    Z_rev_shared = rev_proj(torch.tensor(E_rev_all_s, device=device, dtype=torch.float32)).cpu().numpy()  # normalized shared space\n",
    "\n",
    "\n",
    "Z_hcf_s_all = scaler_hcf.transform(pca_hcf.transform(hcf_scores))\n",
    "with torch.no_grad():\n",
    "    Z_hcf_shared = hcf_proj(torch.tensor(Z_hcf_s_all, device=device, dtype=torch.float32)).cpu().numpy()\n",
    "\n",
    "\n",
    "shared_rev_pairs = Z_rev_shared[pairs_df['review_idx'].to_numpy()]  \n",
    "true_hcf_scores_pairs = hcf_scores[pairs_df['hcf_idx'].to_numpy()] \n",
    "\n",
    "\n",
    "calibrator = Ridge(alpha=1.0)\n",
    "calibrator.fit(shared_rev_pairs, true_hcf_scores_pairs)\n",
    "\n",
    "\n",
    "def predict_hcf_for_review_contrastive(review_row):\n",
    "\n",
    "    text = \" \".join([str(review_row.get(f, \"\")) for f in text_fields])\n",
    "\n",
    "    emb = get_text_embedding([text]) \n",
    "\n",
    "    emb_s = scaler_rev.transform(emb)  \n",
    "    with torch.no_grad():\n",
    "\n",
    "        shared = rev_proj(torch.tensor(emb_s, device=device, dtype=torch.float32))\n",
    "        shared = shared.cpu().numpy() \n",
    "\n",
    "    raw_pred = calibrator.predict(shared) \n",
    "    return pd.Series(raw_pred.flatten(), index=hcf_score_cols)\n",
    "\n",
    "\n",
    "def calibrate_with_overlap(df_overlap):\n",
    "    X_base = []\n",
    "    y_true = []\n",
    "    for _, row in df_overlap.iterrows():\n",
    "        pred = predict_hcf_for_review_contrastive(row).to_numpy()\n",
    "        X_base.append(pred)\n",
    "        y_true.append(row[hcf_score_cols].to_numpy())\n",
    "    X_base = np.vstack(X_base)\n",
    "    y_true = np.vstack(y_true)\n",
    "    residual_model = Ridge(alpha=1.0)\n",
    "    residual_model.fit(X_base, y_true)\n",
    "    return residual_model  \n",
    "    \n",
    "preds_on_pairs = calibrator.predict(shared_rev_pairs)\n",
    "rmse_pseudo = np.sqrt(mean_squared_error(true_hcf_scores_pairs, preds_on_pairs))\n",
    "print(\"Contrastive pipeline pseudo-pair RMSE:\", rmse_pseudo)\n",
    "\n",
    "example = text17_df.iloc[0]\n",
    "print(\"Contrastive HCF prediction (review):\\n\", predict_hcf_for_review_contrastive(example))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb0261",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "shared_rev_pairs = Z_rev_shared[pairs_df['review_idx'].to_numpy()]  \n",
    "true_hcf_scores_pairs = hcf_scores[pairs_df['hcf_idx'].to_numpy()] \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    shared_rev_pairs, true_hcf_scores_pairs, test_size=0.15, random_state=0\n",
    ")\n",
    "\n",
    "def explained_fraction(original, recon):\n",
    "    var = np.var(original, axis=0, ddof=1)\n",
    "    rmse_per_score = np.sqrt(mean_squared_error(original, recon, multioutput='raw_values'))\n",
    "    return 1 - (rmse_per_score**2) / var  # can be negative\n",
    "\n",
    "\n",
    "ridge_cal = Ridge(alpha=1.0).fit(X_train, y_train)\n",
    "mlp_cal = MLPRegressor(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    activation='relu',\n",
    "    early_stopping=True,\n",
    "    random_state=0,\n",
    "    max_iter=500,\n",
    "    learning_rate_init=1e-3,\n",
    "    tol=1e-4,\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "def eval_calibrator(model, name):\n",
    "    pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    expl = explained_fraction(y_val, pred)\n",
    "    print(f\"{name} RMSE (val): {rmse:.4f}\")\n",
    "    print(f\"{name} explained fraction per score (val): {expl}\")\n",
    "    print(f\"{name} avg explained frac: {expl.mean():.4f}\")\n",
    "    return pred\n",
    "\n",
    "\n",
    "pred_ridge = eval_calibrator(ridge_cal, \"Ridge\")\n",
    "pred_mlp = eval_calibrator(mlp_cal, \"MLP\")\n",
    "\n",
    "\n",
    "best_calibrator = mlp_cal if explained_fraction(y_val, pred_mlp).mean() >= explained_fraction(y_val, pred_ridge).mean() else ridge_cal\n",
    "best_name = \"MLP\" if best_calibrator is mlp_cal else \"Ridge\"\n",
    "print(f\"Selected best calibrator: {best_name}\")\n",
    "\n",
    "\n",
    "residual_model = None\n",
    "def fit_residual_overlap(df_overlap):\n",
    "    global residual_model\n",
    "    X_overlap = []\n",
    "    y_overlap = []\n",
    "    for _, row in df_overlap.iterrows():\n",
    "\n",
    "        shared = rev_proj(torch.tensor(scaler_rev.transform(get_text_embedding(\n",
    "            [\" \".join([str(row.get(f, \"\")) for f in text_fields])])[0].reshape(1, -1)\n",
    "        ), device=device, dtype=torch.float32))\n",
    "        shared_np = shared.cpu().numpy().reshape(1, -1)\n",
    "        base_pred = best_calibrator.predict(shared_np)  # (1,7)\n",
    "        X_overlap.append(base_pred.flatten())\n",
    "        y_overlap.append(row[hcf_score_cols].to_numpy())\n",
    "    X_overlap = np.vstack(X_overlap)\n",
    "    y_overlap = np.vstack(y_overlap)\n",
    "    residual_model = Ridge(alpha=1.0).fit(X_overlap, y_overlap)\n",
    "    print(\"Fitted residual correction on real overlap.\")\n",
    "    return residual_model  # final prediction: residual_model(best_calibrator(...))\n",
    "\n",
    "\n",
    "def predict_hcf_with_nonlinear(review_row):\n",
    "\n",
    "    text = \" \".join([str(review_row.get(f, \"\")) for f in text_fields])\n",
    "    emb = get_text_embedding([text])\n",
    "    emb_s = scaler_rev.transform(emb)\n",
    "    with torch.no_grad():\n",
    "        shared = rev_proj(torch.tensor(emb_s, device=device, dtype=torch.float32)).cpu().numpy()\n",
    "    base_pred = best_calibrator.predict(shared.reshape(1, -1)).flatten()\n",
    "    if residual_model is not None:\n",
    "        corrected = residual_model.predict(base_pred.reshape(1, -1)).flatten()\n",
    "        return pd.Series(corrected, index=hcf_score_cols)\n",
    "    else:\n",
    "        return pd.Series(base_pred, index=hcf_score_cols)\n",
    "\n",
    "example = text17_df.iloc[0]\n",
    "print(f\"Final predicted HCF scores (nonlinear calibrated) for example:\\n{predict_hcf_with_nonlinear(example)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a770bd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d13a7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
